# RoToR: Towards More Reliable Responses for Orderâ€‘Invariant Inputs

- Accepted to **ACLÂ 2025 (main)**  [(arXiv link)](https://arxiv.org/abs/2502.08662)
- This repository provides the **official implementation** of RoToR together with baselines (Orig,â€¯PINE,â€¯PCW). It supports evaluation on MMLU, KGQA (Mintaka), Lostâ€‘inâ€‘theâ€‘Middle, Selective Routing, and templateâ€‘swap experiments.
- (25.6.15) Presentation slides and posters are scheduled to be uploaded within a week!

---


## 0. Table of Contents

1. [Quick Start](#quick-start)
2. [Supported Models](#supported-models)
3. [Supported Methods](#supported-methods)
4. [Datasets & Commands](#datasets--commands)
   - [KGQA (Mintaka)](#kgqa-mintaka)  
   - [MMLU](#mmlu)  
   - [Lost-in-the-Middle (LitM)](#lost-in-the-middle-litm)  
   - [Template Swap](#variant-template-swap-experiment)
5. [Environment](#environment)
6. [Directory Layout](#directory-layout)
7. [Third-party Components & Licences](#third-party-components--license)
8. [Citation](#citation)

---

## 1. Quickâ€¯Start <a id="quick-start"></a>

All commands are executed from `src/`. Replace **`name_of_exp`** and other placeholders as needed.

```bash
# ExampleÂ ( MMLU, Orig, logâ€‘likelihood inference )

CUDA_VISIBLE_DEVICES=0 \
python3 -m src.run \
    --name name_of_exp \
    --data mmlu \
    --model_name Qwen/Qwen1.5-4B-Chat \
    --method orig \
    --inference_type log_likelihood \
    --mode 0           # see Â§â€¯MMLU for mode â†” order mapping
```

Other example scripts are in **`src/scripts/`**.

---

## 2. Supportedâ€¯Models <a id="supported-models"></a>

| Family                 | `--model_name` value                                                        |
| ---------------------- | --------------------------------------------------------------------------- |
| **Qwen1.5â€‘Chat**       | `Qwen/Qwen1.5-4B-Chat`<br>`Qwen/Qwen1.5-7B-Chat`<br>`Qwen/Qwen1.5-72B-Chat` |
| **Llamaâ€‘3.1â€‘Instruct** | `meta-llama/Llama-3.1-8B-Instruct`<br>`meta-llama/Llama-3.1-70B-Instruct`   |

---

## 3. Supportedâ€¯Methods <a id="supported-methods"></a>

| Method    | Flag(s)         | KeyÂ Options                                     |
| --------- | --------------- | ----------------------------------------------- |
| **Orig.** | `--method orig` | â€”                                               |
| **RoToR** | `--method ours` | `--sorting_method {lexicalÂ \|Â monot5Â \|Â freq}`  |
| **PINE**  | `--method pine` | â€”                                               |
| **PCW**   | `--method pcw`  | `--pcw_window_k 4â€¯(mmlu) / 10 / 20 / 30â€¯(LitM)` |

---

## 4. Datasetsâ€¯&â€¯Commands <a id="datasets--commands"></a>

### 4-1. KGQAÂ (Mintaka) <a id="kgqa-mintaka"></a>

* **Location**â€¯`src/data_wrapper/kgqa_data/`
  Examples: `mintaka_shuffle1.json`, `mintaka_shuffle0_top50.json`
* Generated with the [KALMV](https://github.com/JinheonBaek/KALMV) pipeline (an improved version of KAPING, which can be run by removing verifier options).

```bash
CUDA_VISIBLE_DEVICES=0 \
python -m src.run \
    --model_name Qwen/Qwen1.5-4B-Chat \
    --name exp_name \
    --data mintaka \
    --split 30 \
    --method orig
```

* `--split`â€¯**30**Â orÂ **50**
* `--measure_flops`â€¯(optional)
* `--mode random_shuffle --seed {0â€¯|â€¯1â€¯|â€¯2}`â€¯(to run afterâ€‘shuffle variants)

#### Variant: Template Swap Experiment <a id="variant-template-swap-experiment"></a>

Add `--mode template_swap`: changes the instruction text (Appendix K at main paper)

---

### 4-2. MMLU <a id="mmlu"></a>

* Cached JSONL located at `src/data_wrapper/mmlu_cache.jsonl` (produced via [lmâ€‘evaluationâ€‘harness](https://github.com/EleutherAI/lm-evaluation-harness)).

```bash
CUDA_VISIBLE_DEVICES=0 \
python3 run.py \
    --name exp_name \
    --data mmlu \
    --model_name meta-llama/Meta-Llama-3.1-8B-Instruct \
    --split mmlu_full \
    --method orig \
    --inference_type log_likelihood \
    --mode 0          # original order
```

`--mode 0` indicates 0,1,2,3 question order (original), and `--mode 23` indicates 3,2,1,0 question order (reversed).

```
0: 0 1 2 3     1: 0 1 3 2     2: 0 2 1 3     3: 0 2 3 1
4: 0 3 1 2     5: 0 3 2 1     6: 1 0 2 3     7: 1 0 3 2
8: 1 2 0 3     9: 1 2 3 0    10: 1 3 0 2    11: 1 3 2 0
12: 2 0 1 3   13: 2 0 3 1    14: 2 1 0 3    15: 2 1 3 0
16: 2 3 0 1   17: 2 3 1 0    18: 3 0 1 2    19: 3 0 2 1
20: 3 1 0 2   21: 3 1 2 0    22: 3 2 0 1    23: 3 2 1 0
```

#### Selectiveâ€¯Routing

1. Run **Orig** and **RoToR** with the **same `--name` and `--mode`** to cache outputs.
2. Reâ€‘run the RoToR command with
   `--routing_alpha 0.2 --routing`.
3. Bulk evaluation helper: `mmlu_eval_bulk.py`. (We used bulk evaluation to report our experiments on paper)

---

### 4-3. Lostâ€‘inâ€‘theâ€‘MiddleÂ (LitM) <a id="lost-in-the-middle-litm"></a>

Dataset &â€¯prompts adapted from the original [lostâ€‘inâ€‘theâ€‘middle](https://github.com/nelson-liu/lost-in-the-middle) repository.

```bash
CUDA_VISIBLE_DEVICES=0 \
python -m src.run \
    --model_name meta-llama/Meta-Llama-3.1-8B-Instruct \
    --name exp_name \
    --data lostinthemiddle \
    --method {orig|pine|ours} \
    --split {10|20|30} \
    --mode no_indexing \
    --subsplit {0|4|9|...}
```


* Always pass `--mode no_indexing` to match mainâ€‘text results.
  (Omit the flag to replicate AppendixÂ A which prefixes documents with indices.)
* **Combinations**
    * splitâ€¯10Â â†’â€¯subsplitâ€¯0â€¯/â€¯4â€¯/â€¯9
    * splitâ€¯20Â â†’â€¯subsplitâ€¯0â€¯/â€¯4â€¯/â€¯9â€¯/â€¯14â€¯/â€¯19
    * splitâ€¯30Â â†’â€¯subsplitâ€¯0â€¯/â€¯4â€¯/â€¯9â€¯/â€¯14â€¯/â€¯19â€¯/â€¯24â€¯/â€¯29
      (`split`Â = total documents, `subsplit`Â = goldâ€‘doc position)

---

## 5. Environment <a id="environment"></a>

Experiments are run on a docker with base image: `pytorch/pytorch:2.2.0-cuda12.1-cudnn8-devel`. Install the **modelâ€‘specific** versions shown below.

### Llamaâ€‘3.1â€‘Instruct setup

```bash
pip install torch==2.2.2+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121
pip install flash-attn==2.5.0 --no-build-isolation
pip install datasets==2.21.0
pip install bitsandbytes==0.43.1
pip install transformers==4.43.1 accelerate sentencepiece einops
```

### Qwen1.5-Chat setup

```bash
# Torch: 2.3 is recommended, 2.0.1 is also known-good
pip install torch==2.3.0+cu121 --index-url https://download.pytorch.org/whl/cu121

# Core libraries
pip install datasets==2.21.0  transformers==4.40.0  peft==0.10.0
pip install flash-attn==2.1.0
```

> **Note**
>
> * The Qwen stack must use **`transformers==4.40.0`** (newer versions break Qwen compatibility).
> * A convenience file, **`requirements_qwen.txt`**, is provided for reproducible installsâ€”though some listed packages may be optional depending on your experiment.



---

## 6. Directoryâ€¯Layout <a id="directory-layout"></a>

```
RoToR/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ PINE/                     # Mechanistic position-bias baseline & Implementation of RoToR
â”‚   â””â”€â”€ pine/
â”‚       â”œâ”€â”€ llama/            # Llama checkpoints & model defs
â”‚       â”‚   â”œâ”€â”€ modeling_llama_orig.py
â”‚       â”‚   â”œâ”€â”€ modeling_llama_rotor.py
â”‚       â”‚   â””â”€â”€ tokenization_llama.py
â”‚       â””â”€â”€ qwen2/            # Qwen counterparts (same file pattern)
â”‚
â”œâ”€â”€ PCW/                      # Parallel-Context-Windows (vendored)
â”‚
â”œâ”€â”€ lost-in-the-middle/       # LitM dataset & helpers (vendored)
â”‚   â”œâ”€â”€ setup.py              # install with `pip install -e .`
â”‚   â””â”€â”€ qa_data/              # pre-processed QA splits
â”‚
â”œâ”€â”€ outputs/                      # path to save run output
â”‚
â””â”€â”€ src/                        # RoToR driver code
    â”œâ”€â”€ run.py                 # main experiment entry point
    â”œâ”€â”€ ordering_strategy.py    # lexical / monoT5 / freq sorters
    â”œâ”€â”€ scripts/               # convenience launch scripts
    â””â”€â”€ data_wrapper/          # dataset-specific wrappers & caches
        â”œâ”€â”€ litm_wrapper.py      # utilities for Lost-in-the-Middle
        â”œâ”€â”€ kgqa_wrapper.py      # utilities for Mintaka KGQA
        â”œâ”€â”€ mmlu_wrapper.py      # utilities for MMLU
        â””â”€â”€ kgqa_data/         # pre-processed Mintaka splits (JSON)
```

* Inside `lost-in-the-middle/` run:

```bash
pip install -e .
```

- **Flash-Attn compatibility**: lost-in-the-middle may pull a newer Flash-Attn version that breaks RoToR. Immediately downgrade to v2.0.1:
```
pip install flash-attn==2.0.1
```

---

## 7. ðŸ“¦ Thirdâ€‘party Components &â€¯License <a id="third-party-components--license"></a>

| Component                          | Upstream                                                                                                     | Licence    | Notes                                                                                                                                 |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| **PINE**                           | [https://github.com/wzq016/PINE](https://github.com/wzq016/PINE)                                             | MIT        | Added `modeling_qwen2_rotor.py`, `modeling_llama_rotor.py`; original files renamed `*-orig.py` under `PINE/pine/models/{llama,qwen}`. |
| **lostâ€‘inâ€‘theâ€‘middle**             | [https://github.com/nelson-liu/lost-in-the-middle](https://github.com/nelson-liu/lost-in-the-middle)         | MIT        | Vendored under `lost-in-the-middle/`; see `third_party/litm/LICENSE`.                                                                 |
| **Parallelâ€‘Contextâ€‘Windows (PCW)** | [https://github.com/AI21Labs/Parallel-Context-Windows](https://github.com/AI21Labs/Parallel-Context-Windows) | ApacheÂ 2.0 | Vendored under `PCW/`; **currently not runnable** due to dependency conflicts.                                                        |

---

## Citation <a id="citation"></a>

If you use RoToR or the accompanying code, please cite:

```bibtex
@misc{yoon2025rotorreliableresponsesorderinvariant,
  title        = {RoToR: Towards More Reliable Responses for Order-Invariant Inputs},
  author       = {Soyoung Yoon and Dongha Ahn and Youngwon Lee and Minkyu Jung and HyungJoo Jang and Seung-won Hwang},
  year         = {2025},
  eprint       = {2502.08662},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2502.08662}
}
```

---

Happy experimenting! For questions or issues, please feel free to email `soyoung.yoon@snu.ac.kr` or open an github issue.
